{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:49:08.174519Z",
          "start_time": "2020-03-31T13:49:08.165989Z"
        },
        "id": "gpbvNOH0zvIi"
      },
      "source": [
        "# **Competencia 1 - CC6205 Natural Language Processing 📚**\n",
        "\n",
        "Departamento de Ciencias de la Computación, Universidad de Chile.\n",
        "\n",
        "CC6205: Procesamiento de Lenguaje Natural - Otoño 2023\n",
        "\n",
        "**Integrantes:** [inserten sus nombres aquí]\n",
        "\n",
        "**Usuario del equipo en CodaLab:** [inserten su usuario aquí]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxlNrNf_p0ZY"
      },
      "source": [
        "## **Objetivo**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrtvsKf2p3A4"
      },
      "source": [
        "En esta tarea grupal participarán de una competencia al estilo [Kaggle](https://www.kaggle.com/) pero utilizando la página [CodaLab](https://codalab.org/). El objetivo de la competencia es predecir si un texto contiene discurso de odio, incivilidad o es un texto normal.\n",
        "\n",
        "El discurso de odio es cualquier expresión que promueva o incite a la discriminación, la hostilidad o la violencia hacia una persona o grupo de personas en una relación asimétrica de poder, tal como la raza, la etnia, el género, la orientación sexual, la religión, la nacionalidad, una discapacidad u otra característica similar.\n",
        "\n",
        "En cambio, la incivilidad se refiere a cualquier comportamiento o actitud que rompe las normas de respeto, cortesía y consideración en la interacción entre personas. Esta puede manifestarse de diversas formas, tal como insultos, ataques personales, sarcasmo, desprecio, entre otras.\n",
        "\n",
        "En esta competencia tendrán a su disposición un dataset de textos con las etiquetas `odio`, `incivilidad` o `normal`. La mayor parte de los datos se encuentra en español de Chile. Con estos datos, deberán entrenar un modelo que sea capaz de predecir la etiqueta de un texto dado.\n",
        "\n",
        "El corpus para esta tarea se compone de 3 datasets:  \n",
        "- [Multilingual Resources for Offensive Language Detection de Arango et al. (2022)](https://aclanthology.org/2022.woah-1.pdf#page=136)\n",
        "- [Dataton UTFSM No To Hate (2022)](http://dataton.inf.utfsm.cl/)\n",
        "- Datos generados usando la [API de GPT3 (modelo DaVinci 03)](https://platform.openai.com/docs/models/gpt-3).\n",
        "\n",
        "Agradecimientos a los autores por compartir los datos y a David Miranda, Fabián Diaz, Santiago Maass y Jorge Ortiz por revisar y reetiquetar los datos en el contexto del curso \"Taller de Desarrollo de Proyectos de IA\" (CC6409), Departamento de Ciencias de la Computación, Universidad de Chile. \n",
        "\n",
        "Los datos solo pueden ser usados con fines de investigación y docencia. Está prohibida la difusión externa.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6lhhfl2zvIk"
      },
      "source": [
        "## **Instrucciones**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T14:34:38.796217Z",
          "start_time": "2020-04-07T14:34:38.782255Z"
        },
        "id": "7wTyult1zvIl"
      },
      "source": [
        "- La competencia consiste en resolver el problema de clasificación presentado. La evaluación de la competencia se realiza en base a 3 métricas: `AUC`, `Kappa` y `Accuracy`. Los mejores puntajes en cada una de estas métricas serán quienes ganen la competencia.\n",
        "\n",
        "- Para comenzar se les entregará en este notebook la estructura del informe a entregar y el código de un baseline con el cuál pueden comparar los resultados de sus experimentos. Este baseline consiste en la creación de features y una clasificación simple siguiendo el paradigma de Machine Learning empírico. Los puntajes obtenidos por el baseline los pueden visualizar en el link de CodaLab buscando al usuario **cc6205**. Esperamos que los superen fácilmente 😉\n",
        "\n",
        "- Para participar, deben registrarse en CodaLab y luego ingresar a la competencia usando el enlace que será provisto a través de U-Cursos.\n",
        "\n",
        "- Está permitido hacer grupos de máximo 4 alumnos. Cada grupo debe tener un nombre de equipo, para ello en CodaLab pueden dirigirse a settings y luego cambiar el Team Name. Sólo una persona debe administrar la cuenta del grupo y se verificará que no se hayan creado múltiples cuentas por grupo.\n",
        "\n",
        "- En total pueden hacer un **máximo de 5 submissions**, hagan muchos experimentos probando en el conjunto de test antes de realizar el envío.\n",
        "\n",
        "- Es importante que hagan varios experimentos incorporando técnicas como [cross-validation](https://es.wikipedia.org/wiki/Validaci%C3%B3n_cruzada#:~:text=La%20validaci%C3%B3n%20cruzada%20o%20cross,datos%20de%20entrenamiento%20y%20prueba.), [random sampling](https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c) o [bayesian optimization](https://optuna.org/) antes de enviar sus predicciones a CodaLab, ya que les puede dar un mejor indicio del nivel de generalización de sus modelos.\n",
        "\n",
        "- Asegúrense de que la distribución de las clases se mantenga en las particiones de training y testing.\n",
        "\n",
        "- Verificar que el formato de la submission coincida con el de la competencia. De lo contrario, se les será evaluado incorrectamente ya que el Script de evaluación espera como input dicho formato. En el código de las métricas pueden verificar cómo son los inputs.\n",
        "\n",
        "- No se limiten a los contenidos vistos ni a scikit ni a este baseline. No tienen restricciones entre utilizar Deep Learning o Machine Learning empírico. Si reutilizan gran cantidad de código de alguna página por favor mostrar la referencia en su código. ¡Usen todo su conocimiento e ingenio en mejorar sus sistemas para poder ganar la competencia!\n",
        "\n",
        "- **Es requisito entregar el reporte con el código y haber participado en la competencia para ser evaluado. Un código sin reporte o un reporte sin código serán evaluados con la nota mínima.**\n",
        "\n",
        "- Todas las dudas escríbanlas en el canal de Discord de competencias. Los emails que lleguen al equipo docente serán remitidos a ese medio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:25:19.677190Z",
          "start_time": "2020-04-07T15:25:19.671206Z"
        },
        "id": "jiDISxa-zvIn"
      },
      "source": [
        "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) en el reporte. NO serán evaluados Notebooks sin nombre.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igf7TBfSzvIo"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6moqxkEwCe-"
      },
      "source": [
        "## **Reporte a entregar**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo7vfXV4wD8s"
      },
      "source": [
        "Uno de los puntos claves en la evaluación de esta competencia es elaborar un informe claro y preciso, argumentando las decisiones tomadas al momento de crear sus modelos para que el cuerpo docente pueda entenderlos. La estructura que debe contener es la siguiente:\n",
        "\n",
        "1. **Introducción**: Presentar brevemente el problema a resolver, incluyendo la formalización de la task (cómo son los inputs y outputs del problema) y los desafíos que ven al analizar el corpus entregado. (**0.5 puntos**)\n",
        "2. **Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores, **recordar** que para entrenar modelos el input debe tener su representación numérica. Si bien, con Bag of Words (**baseline**) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluación agregando más atributos y representaciones diseñadas a mano, sean lo más creativos posible. Más abajo encontrarán una lista de estos posibles atributos que les podrá ser de utilidad. (**1 punto**)\n",
        "3. **Algoritmos**: Describir **brevemente** los algoritmos de clasificación usados, tanto si fueron algoritmos ya vistos en clases o bien arquitecturas de Deep Learning. (**0.5 puntos**)\n",
        "4. **Métricas de evaluación**: Describir brevemente las métricas utilizadas en la evaluación, indicando qué miden y su interpretación. (**0.5 puntos**)\n",
        "\n",
        "5. **Diseño experimental**: Esta es una de las secciones más importantes del reporte. Deben describir minuciosamente los experimentos que realizarán en la siguiente sección. Describir las variables de control que manejarán, algunos ejemplos pueden ser: Los parámetros de los clasificadores, parámetros en las funciones con que procesan los textos y los transforman, parámetros para el cross-validation, particiones de datos utilizadas, etc. En caso que utilicen redes neuronales, ser claros con el conjunto de hiperparámetros que probarán, la decisión en las funciones de optimización, función de pérdida, regulación, etc. Básicamente explicar qué es lo que veremos en la siguiente sección.\n",
        "   (**1 punto**)\n",
        "\n",
        "6. **Experimentos**: Incluyan todo el código de sus experimentos aquí. ¡Es vital haber realizado varios experimentos para sacar una buena nota! (**1.5 puntos**)\n",
        "\n",
        "7. **Resultados**: Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones. Pueden mostrar los resultados sobre la partición de validación en caso que la generen o sobre los resultados del conjunto de testing. Mostrar los resultados en alguna tabla, pueden poner aquí también los resultados obtenidos al realizar la submission. (**0.5 puntos**)\n",
        "\n",
        "8. **Conclusiones**: Discutir resultados, proponer trabajo futuro. (**0.5 puntos**)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMSn_tDYwOb1"
      },
      "source": [
        "## **Descripción Baseline**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T19:18:43.301002Z",
          "start_time": "2019-08-21T19:18:43.298037Z"
        },
        "id": "30fPWG5pzvIm"
      },
      "source": [
        "El baseline de la sección **Experimentos** contiene un código básico que resuelve la task y es el modelo subido por **cc6205-baseline** a la competencia. Pueden modificar el código como quieran siempre y cuando no cambien las funciones de las métricas y el formato en que se suben los archivos a la competencia, ya que ese es el formato en que realizamos la evaluación. En concretro, el baseline hace lo siguiente:\n",
        "\n",
        "- Obtiene los datasets desde el repositorio del curso.\n",
        "- Divide los datasets en train (datos que están etiquetados) y target set (datos no etiquetados para la competencia). Además, por cada dataset en train, se divide en un conjunto de entrenamiento y uno de prueba. Aquí la mejor práctica sería cambiar el código para obtener un conjunto de entrenamiento, validación y prueba.\n",
        "\n",
        "- Crea un Pipeline que:\n",
        "  - Crea features personalizados para la representación numérica.\n",
        "  - Transforma los dataset a bag of words (BoW).\n",
        "  - Entrena un clasificador usando cada train set.\n",
        "- Clasifica y evalua el sistema creado usando el test set.\n",
        "- Clasifica el target set.\n",
        "- Genera una submission con el target en formato zip en el directorio en donde se está ejecutando el notebook.\n",
        "\n",
        "Algunas pistas sobre como mejorar el rendimiento de los sistemas que creen. (Esto tendrá mas sentido cuando vean el código)\n",
        "\n",
        "- **Datos**: Pueden utilizar técnicas de preprocesamiento y data augmentation para mejorar sus resultados. Algunos paquetes útiles para ello son:\n",
        "\n",
        "  - [nlpaug](https://github.com/makcedward/nlpaug)\n",
        "  - [spanish_nlp](https://github.com/jorgeortizfuentes/spanish_nlp)\n",
        "  - [spacy](https://spacy.io/usage/spacy-101)\n",
        "\n",
        "- **Vectorizador**: Investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. También, el parámetro `ngram_range` (Ojo que el clf naive bayes no debería usarse con n-gramas, ya que rompe el supuesto de independencia). Además, implementar los atributos que crean útiles desde el listado del enunciado. Investigar también el vectorizador tf-idf.\n",
        "\n",
        "- **Clasificador**: Investigar otros clasificadores más efectivos que Naive Bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la función `predict_proba`). Algunos clasificadores que pueden probar son:\n",
        "\n",
        "  - SVM\n",
        "  - Random Forest\n",
        "  - XGBoost\n",
        "  - Redes Neuronales (MLP)\n",
        "\n",
        "- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Aquí les adjuntamos algunos ejemplos:\n",
        "\n",
        "  - N-gramas de palabras.\n",
        "  - N-gramas de caracteres.\n",
        "  - Part-of-speech tags.\n",
        "  - Lexicones de sentimiento (Lexicon = Conjunto de palabras con una etiqueta o valor asociado).\n",
        "    - Cuenta el número de palabras positivas y negativas dentro de una frase.\n",
        "    - Si el léxico tiene asociada la intensidad del sentimiento (por ejemplo, en un decimal), saque la media de la intensidad de la frase según el sentimiento, la suma, etc.\n",
        "    - Pueden encontrar un lexicon en [este siguiente enlace](https://github.com/kicorangel/RD-Lab/tree/master/resources/Afffectivity/Spanish%20Opinion%20Lexicon)\n",
        "  - El número de palabras alargadas (palabras con un carácter repetido más de dos veces).\n",
        "  - El número de palabras con todos los caracteres en mayúsculas.\n",
        "  - La presencia y el número de emoticonos positivos o negativos.\n",
        "  - El número de negaciones individuales.\n",
        "  - El número de secuencias contiguas de puntos, signos de interrogación y exclamación.\n",
        "  - Word Embeddings: Aquí tienen algunos modelos preentrenados para vectorizar los textos.\n",
        "    - [Spanish Word Embeddings](https://github.com/dccuchile/spanish-word-embeddings)\n",
        "    - [Sentence BERT](https://www.sbert.net/docs/quickstart.html)\n",
        "\n",
        "- **Reducción de dimensionalidad**: También puede serles de ayuda. Referencias [aquí](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n",
        "\n",
        "- Por último, pueden encontrar mas referencias de cómo mejorar sus features, el vectorizador y el clasificador [aquí](https://affectivetweets.cms.waikato.ac.nz/benchmark/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT7ZpVRuzGAF"
      },
      "source": [
        "# **Entregable.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:34:25.683540Z",
          "start_time": "2020-03-31T13:34:25.673430Z"
        },
        "id": "E29LEMZ9zvIo"
      },
      "source": [
        "## **1. Introducción**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W20NnoduzvIo"
      },
      "source": [
        "    Escriba su introducción aquí\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:13.474238Z",
          "start_time": "2020-03-31T13:47:13.454068Z"
        },
        "id": "OTAIEnSJzvIp"
      },
      "source": [
        "## **2. Representaciones**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:17.719268Z",
          "start_time": "2020-03-31T13:47:17.709207Z"
        },
        "id": "EV1qBv-MzvIp"
      },
      "source": [
        "    Escriba sus decisiones para la representación\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMOjYSQezvIq"
      },
      "source": [
        "## **3. Algoritmos**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bPiFs33zilS"
      },
      "source": [
        "    Explique los algoritmos utilizados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:52.064631Z",
          "start_time": "2020-03-31T13:47:52.044451Z"
        },
        "id": "ECjkdgdwzvIq"
      },
      "source": [
        "## **4. Métricas de Evaluación**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6eHJdHBzvIr"
      },
      "source": [
        "- AUC: ...\n",
        "- Kappa: ...\n",
        "- Accuracy: ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJyTrr2onLOo"
      },
      "source": [
        "## **5. Diseño experimental**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnhEwjh_nP9M"
      },
      "source": [
        "    Descripción de la metodología\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX5Ib_pCzvIr"
      },
      "source": [
        "## **6. Experimentos**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:31:40.023344Z",
          "start_time": "2020-03-31T13:31:40.003541Z"
        },
        "id": "aK24MJ8jzvIr"
      },
      "source": [
        "### Importar librerías\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.587160Z",
          "start_time": "2020-04-07T15:44:19.319386Z"
        },
        "id": "FukgFUTUzvIs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    cohen_kappa_score,\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FevBPus0zvIs"
      },
      "source": [
        "### Definir métodos de evaluación (**NO tocar este código**)\n",
        "\n",
        "Estas funciones están a cargo de evaluar los resultados de la tarea. No deberían cambiarlas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.604066Z",
          "start_time": "2020-04-07T15:44:20.589106Z"
        },
        "id": "9wlllV7PzvIs"
      },
      "outputs": [],
      "source": [
        "def auc_score(test_set, predicted_set):\n",
        "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
        "    medium_predicted = np.array(\n",
        "        [prediction[1] for prediction in predicted_set]\n",
        "    )\n",
        "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
        "    inc_test = np.where(test_set == \"incivilidad\", 1.0, 0.0)\n",
        "    odio_test = np.where(test_set == \"odio\", 1.0, 0.0)\n",
        "    normal_test = np.where(test_set == \"normal\", 1.0, 0.0)\n",
        "    auc_high = roc_auc_score(inc_test, high_predicted)\n",
        "    auc_med = roc_auc_score(odio_test, medium_predicted)\n",
        "    auc_low = roc_auc_score(normal_test, low_predicted)\n",
        "    auc_w = (\n",
        "        normal_test.sum() * auc_low\n",
        "        + odio_test.sum() * auc_med\n",
        "        + inc_test.sum() * auc_high\n",
        "    ) / (normal_test.sum() + odio_test.sum() + inc_test.sum())\n",
        "    return auc_w\n",
        "\n",
        "\n",
        "def evaluate(predicted_probabilities, y_test, labels):\n",
        "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
        "    # entregar el arreglo de clases aprendido por el clasificador.\n",
        "    # (que comunmente, es distinto a ['normal', 'odio', 'incivilidad'])\n",
        "    predicted_labels = [\n",
        "        labels[np.argmax(item)] for item in predicted_probabilities\n",
        "    ]\n",
        "\n",
        "    print(\"Matriz de confusión\")\n",
        "    print(\n",
        "        confusion_matrix(\n",
        "            y_test, predicted_labels, labels=[\"normal\", \"odio\", \"incivilidad\"]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(\"\\nReporte de clasificación:\\n\")\n",
        "    print(\n",
        "        classification_report(\n",
        "            y_test, predicted_labels, labels=[\"normal\", \"odio\", \"incivilidad\"]\n",
        "        )\n",
        "    )\n",
        "    # Reorder predicted probabilities array.\n",
        "    labels = labels.tolist()\n",
        "\n",
        "    predicted_probabilities = predicted_probabilities[\n",
        "        :,\n",
        "        [\n",
        "            labels.index(\"normal\"),\n",
        "            labels.index(\"odio\"),\n",
        "            labels.index(\"incivilidad\"),\n",
        "        ],\n",
        "    ]\n",
        "\n",
        "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
        "    print(\"Métricas:\\n\\nAUC: \", auc, end=\"\\t\")\n",
        "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
        "    print(\"Kappa:\", kappa, end=\"\\t\")\n",
        "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"------------------------------------------------------\\n\")\n",
        "    return np.array([auc, kappa, accuracy])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkOP6ugwzvIt"
      },
      "source": [
        "### Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.068137Z",
          "start_time": "2020-04-07T15:44:20.606061Z"
        },
        "id": "D1XhFPhrzvIt"
      },
      "outputs": [],
      "source": [
        "# Dataset de entrenamiento.\n",
        "train = pd.read_csv(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/new/assignment_1/train/train.tsv\", sep=\"\\t\")\n",
        "\n",
        "# Dataset que deberán predecir para la competencia.\n",
        "target = pd.read_csv(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/new/assignment_1/target/target.tsv\", sep=\"\\t\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.088707Z",
          "start_time": "2020-04-07T15:44:21.069757Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "flg2Zw2mzvIt",
        "outputId": "3ada1050-7a4e-4c04-9ff2-60f93fd90332"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                              texto        clase\n",
              "1221   1237  @user a través de su Corporación Cultural invi...       normal\n",
              "4240   5254  Cagaron #comunistas #ctm se quedaron sin pega ...  incivilidad\n",
              "4748   4961  @user Muy cierto.. yo que algo recuerdo de los...  incivilidad\n",
              "5049   9029  Bien bravo esa es hablar de ese pelao ctm de A...  incivilidad\n",
              "1017  15130                              @user Una puta mierda       normal"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24e61fe1-3c91-4e24-aa04-af13ede0128d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>texto</th>\n",
              "      <th>clase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1221</th>\n",
              "      <td>1237</td>\n",
              "      <td>@user a través de su Corporación Cultural invi...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4240</th>\n",
              "      <td>5254</td>\n",
              "      <td>Cagaron #comunistas #ctm se quedaron sin pega ...</td>\n",
              "      <td>incivilidad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4748</th>\n",
              "      <td>4961</td>\n",
              "      <td>@user Muy cierto.. yo que algo recuerdo de los...</td>\n",
              "      <td>incivilidad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5049</th>\n",
              "      <td>9029</td>\n",
              "      <td>Bien bravo esa es hablar de ese pelao ctm de A...</td>\n",
              "      <td>incivilidad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>15130</td>\n",
              "      <td>@user Una puta mierda</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24e61fe1-3c91-4e24-aa04-af13ede0128d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-24e61fe1-3c91-4e24-aa04-af13ede0128d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-24e61fe1-3c91-4e24-aa04-af13ede0128d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Ejemplo de algunas filas aleatorias del dataset etiquetado:\n",
        "train.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XB7hb7KH2DFK",
        "outputId": "f0c783e4-00e1-4cf7-ecc5-990835ff094d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                              texto  clase\n",
              "1439   8221  @user El peor Congreso de la historia, a quien...    NaN\n",
              "1550    648  Porque las lesbianas no son normales. ¿Qué est...    NaN\n",
              "2188  14338  Oe y la @user ¿qué?, y después dicen que no ha...    NaN\n",
              "1738  12622  Tan amariconao que es puch... como no gano se ...    NaN\n",
              "2986    832  Los haitianos no son más que inmigrantes ilega...    NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9c7368b-0389-4144-827e-abfada98de84\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>texto</th>\n",
              "      <th>clase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1439</th>\n",
              "      <td>8221</td>\n",
              "      <td>@user El peor Congreso de la historia, a quien...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1550</th>\n",
              "      <td>648</td>\n",
              "      <td>Porque las lesbianas no son normales. ¿Qué est...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2188</th>\n",
              "      <td>14338</td>\n",
              "      <td>Oe y la @user ¿qué?, y después dicen que no ha...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1738</th>\n",
              "      <td>12622</td>\n",
              "      <td>Tan amariconao que es puch... como no gano se ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2986</th>\n",
              "      <td>832</td>\n",
              "      <td>Los haitianos no son más que inmigrantes ilega...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9c7368b-0389-4144-827e-abfada98de84')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f9c7368b-0389-4144-827e-abfada98de84 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f9c7368b-0389-4144-827e-abfada98de84');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# Ejemplo de algunas filas aleatorias del dataset no etiquetado\n",
        "target.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5aNqEfVzvIv"
      },
      "source": [
        "### Analizar los datos\n",
        "\n",
        "En esta sección analizaremos el balance de los datos. Para ello se imprime la cantidad de tweets de cada dataset agrupados por la intensidad de sentimiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.117633Z",
          "start_time": "2020-04-07T15:44:21.090703Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5007JRgzvIv",
        "outputId": "6e1e06f0-2ac4-4b88-e9c4-2502665a5124"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "incivilidad    5424\n",
              "normal         4280\n",
              "odio           2510\n",
              "Name: clase, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "train[\"clase\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLLFoTQyfZH1",
        "outputId": "f2d28fae-58cb-4718-b36d-10f6ba626c26"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.9/dist-packages (1.3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spanish_nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHYEvPEIeRgJ",
        "outputId": "7b186226-4951-4ff6-b787-7ec657f76a41"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spanish_nlp in /usr/local/lib/python3.9/dist-packages (0.2.9)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.9/dist-packages (from spanish_nlp) (2.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from spanish_nlp) (1.5.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from spanish_nlp) (4.28.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from spanish_nlp) (2.0.0+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from spanish_nlp) (3.8.1)\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.9/dist-packages (from spanish_nlp) (0.2.1)\n",
            "Requirement already satisfied: es-core-news-sm in /usr/local/lib/python3.9/dist-packages (from spanish_nlp) (3.1.0)\n",
            "Requirement already satisfied: swifter in /usr/local/lib/python3.9/dist-packages (from spanish_nlp) (1.3.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from spanish_nlp) (2.11.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (2023.4.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (2.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (23.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (0.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (1.22.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (0.70.14)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (6.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (3.2.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->spanish_nlp) (0.18.0)\n",
            "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from es-core-news-sm->spanish_nlp) (3.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->spanish_nlp) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->spanish_nlp) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->spanish_nlp) (2022.10.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->spanish_nlp) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->spanish_nlp) (2022.7.1)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.9/dist-packages (from swifter->spanish_nlp) (5.9.5)\n",
            "Requirement already satisfied: bleach>=3.1.1 in /usr/local/lib/python3.9/dist-packages (from swifter->spanish_nlp) (6.0.0)\n",
            "Requirement already satisfied: parso>0.4.0 in /usr/local/lib/python3.9/dist-packages (from swifter->spanish_nlp) (0.8.3)\n",
            "Requirement already satisfied: dask[dataframe]>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from swifter->spanish_nlp) (2022.12.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.9/dist-packages (from swifter->spanish_nlp) (7.7.1)\n",
            "Requirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from swifter->spanish_nlp) (2.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->spanish_nlp) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->spanish_nlp) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->spanish_nlp) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->spanish_nlp) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->spanish_nlp) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->spanish_nlp) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->spanish_nlp) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->spanish_nlp) (16.0.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers->spanish_nlp) (0.13.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from bleach>=3.1.1->swifter->spanish_nlp) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach>=3.1.1->swifter->spanish_nlp) (0.5.1)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.9/dist-packages (from dask[dataframe]>=2.10.0->swifter->spanish_nlp) (0.12.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.9/dist-packages (from dask[dataframe]>=2.10.0->swifter->spanish_nlp) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->spanish_nlp) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->spanish_nlp) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->spanish_nlp) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->spanish_nlp) (1.9.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->spanish_nlp) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->spanish_nlp) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->spanish_nlp) (4.0.2)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.0.0->swifter->spanish_nlp) (3.0.7)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.0.0->swifter->spanish_nlp) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.0.0->swifter->spanish_nlp) (5.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.0.0->swifter->spanish_nlp) (5.5.6)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.0.0->swifter->spanish_nlp) (3.6.4)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.0.0->swifter->spanish_nlp) (7.34.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->spanish_nlp) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->spanish_nlp) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->spanish_nlp) (2022.12.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (2.4.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (8.0.17)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (2.0.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (0.7.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (1.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (67.6.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (6.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (3.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-sm->spanish_nlp) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->spanish_nlp) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->spanish_nlp) (1.3.0)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter->spanish_nlp) (6.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter->spanish_nlp) (6.1.12)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (2.14.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (3.0.38)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (4.8.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.18.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.9/dist-packages (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter->spanish_nlp) (1.0.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (6.4.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (1.8.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (1.5.6)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (21.3.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (5.8.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (5.3.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.16.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (6.5.4)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.17.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.2.6)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (3.2.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (21.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (4.9.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.2.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.8.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (4.11.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.7.3)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (4.3.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (0.19.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (2.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter->spanish_nlp) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ3yboxIrDwR",
        "outputId": "913b6af9-c9c8-4d8b-cf91-694e875d0286"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12214, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spanish_nlp import preprocess\n",
        "\n",
        "sp_1 = preprocess.SpanishPreprocess(\n",
        "    lower=True, # Lowercase the text                                                                                         / SÍ\n",
        "    remove_url=True, # Remove URLs                                                                                           / SÍ\n",
        "    remove_hashtags=False, # Remove hashtags                                                                                 / NO, al separarlos puede haber info útil\n",
        "    split_hashtags=True, # Split hashtags into words                                                                         / SÍ, hashtags tienen info útil\n",
        "    normalize_breaklines=True, # Delete multiple breaklines and replace them with a single one                               / SÍ\n",
        "    remove_emoticons=False, # Remove emoticons like :D                                                                       / NO, emoticones denotan sentimientos útiles\n",
        "    remove_emojis=False, # Remove emojis like 😅                                                                             / NO, emojis                \"\n",
        "    convert_emoticons=False, # Convert emoticons like :D to words according to the emoticon dictionary                       / NO, pero quizás haya que hacerlo pq dataset es chico\n",
        "    convert_emojis=False, # Convert emojis like 😅 to words according to the emoji dictionary                                / NO,                       \"\n",
        "    normalize_inclusive_language=True, # Remove inclusive language (to be implemented)                                        / NO, pq aún no está implementado\n",
        "    reduce_spam=False, # Reduces words repeated more than 3 times                                                             / NO, pq repetir palabras exacerba el sentimiento\n",
        "    remove_reduplications=False, # Remove reduplications (to be implemented) like \"holaaaaa\" to \"hola\"                        / NO,       \"     letras         \"\n",
        "    remove_vowels_accents=True, # Convert vowels with accents to their non-accented version                                   / SÍ\n",
        "    remove_multiple_spaces=True, # Normalize multiple spaces to a single one                                                  / SÍ\n",
        "    remove_punctuation=True, # Remove punctuation                                                                             / SÍ\n",
        "    remove_unprintable=True, # Remove unprintable characters                                                                  / SÍ\n",
        "    remove_numbers=True, # Remove numbers                                                                                     / SÍ\n",
        "    remove_stopwords=False, # Remove stopwords                                                                                / NO, haremos una prueba sin removerlas\n",
        "    stopwords_list=None, # List of stopwords to remove (if stopwords_list is None, the default stopwords list will be used)\n",
        "    lemmatize=True, # Lemmatize the text                                                                                      / SÍ, lematizar en más fuerte que stemizar\n",
        "    stem=False, # Stem the text                                                                                               / NO,                   \"        \n",
        "    remove_html_tags=True, # Remove HTML tags                                                                                 / SÍ\n",
        ") "
      ],
      "metadata": {
        "id": "a8v2198Kh4rK"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesamos train y target usando el primer filtro sp_1:\n",
        "\n",
        "train_1 = train\n",
        "\n",
        "for i in train_1.index:\n",
        "  text = train_1.iloc[i, 1]\n",
        "  pr   = sp_1.transform(text, debug=False)\n",
        "  train_1.iloc[i, 1] = pr\n",
        "\n",
        "target_1 = target\n",
        "\n",
        "for i in target_1.index:\n",
        "  text = target_1.iloc[i, 1]\n",
        "  pr   = sp_1.transform(text, debug=False)\n",
        "  target_1.iloc[i, 1] = pr\n",
        "\n",
        "a = train.iloc[11809]['texto']\n",
        "b = train_1.iloc[11809]['texto']\n",
        "c = target.iloc[2304]['texto']\n",
        "d = target_1.iloc[2304]['texto']\n",
        "\n",
        "\n",
        "\n",
        "print(f\"ORIGINAL: \\t{a}\\n PROCESADO \\t{b}\")\n",
        "print(f\"ORIGINAL: \\t{c}\\n PROCESADO \\t{d}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "6uwq4tfgrNnb",
        "outputId": "9461d4a1-5659-4075-d330-e687c94baef8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-74c4260f4abb>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mpr\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0msp_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mtrain_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spanish_nlp/preprocess.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, text, debug)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemmatize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_debug_method_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lemmatize\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spanish_nlp/preprocess.py\u001b[0m in \u001b[0;36m_lemmatize_\u001b[0;34m(self, text, lemmatizer)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_lemmatize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"es_core_news_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;34m\"\"\"Lemmatize text using es_core_news_sm from spacy by default\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \"\"\"\n\u001b[0;32m--> 998\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             )\n\u001b[0;32m-> 1081\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     def update(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/lang/lex_attrs.py\u001b[0m in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stZ6ig5hzvIv"
      },
      "source": [
        "### Custom Features\n",
        "\n",
        "Para crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n",
        "\n",
        "1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta caracteres relevantes ('!', '?', '#', '@') en los tweets.\n",
        "2. Definimos una función cómo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n",
        "3. Hacemos un override de la función `transform` en donde iteramos por cada tweet, llamamos a la función que hicimos antes y agregamos sus resultados a un arreglo. Finalmente lo retornamos.\n",
        "\n",
        "Esto nos facilitará el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aquí](https://scikit-learn.org/stable/data_transforms.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.128600Z",
          "start_time": "2020-04-07T15:44:21.119624Z"
        },
        "id": "tNPB8zc9zvIw"
      },
      "outputs": [],
      "source": [
        "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
        "    def get_relevant_chars(self, tweet):\n",
        "        num_hashtags = tweet.count(\"#\")\n",
        "        num_exclamations = tweet.count(\"!\")\n",
        "        num_interrogations = tweet.count(\"?\")\n",
        "        num_at = tweet.count(\"@\")\n",
        "        return [num_hashtags, num_exclamations, num_interrogations, num_at]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.145564Z",
          "start_time": "2020-04-07T15:44:21.131593Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCQzsuAbzvIw",
        "outputId": "3560d8f0-37e7-43ab-9653-ce37efb030d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textos originales: user y este mapuche flojo ignorante borracho y manipulado decir ese el patita el que hacer el copete yo recordar uno señor varas de uno reality que merito tener ser hombre solo haber comprar el cuento del pc nada mas y ser uno tonto util cosa que el copete el impedir ver\n",
            "Features creados: [0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# Veamos que sucede si ejecutamos el transformer\n",
        "sample = train.sample(5, random_state=42)[\"texto\"]\n",
        "sample_features = CharsCountTransformer().transform(sample)\n",
        "\n",
        "# Se puede verificar que el conteo de símbolos es consistente con el transformer creado.\n",
        "print(f\"Textos originales: {sample.iloc[0]}\")\n",
        "print(f\"Features creados: {sample_features[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WuGhnSAdr8xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM\n",
        "\n"
      ],
      "metadata": {
        "id": "tINy-bNCr9XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.base import clone"
      ],
      "metadata": {
        "id": "4fmMJ4xUsqfR"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RBF(C, gamma, X_t, y_t, X_v, y_v):\n",
        "\n",
        "    #Se eligen los hiperparámetros del rbf_modelo y se toma el tiempo de entrenamiento\n",
        "    hiper_parametros = {'kernel':['rbf'], 'C': C,'gamma': gamma}\n",
        "    svc = SVC()\n",
        "    start = time.perf_counter() \n",
        "    model_grid = GridSearchCV(svc, hiper_parametros)\n",
        "    X_search = np.concatenate((X_t, X_v),axis = 0)\n",
        "    y_search = np.concatenate((y_t, y_v),axis = 0)\n",
        "    model_grid.fit(X_search, y_search)\n",
        "    best = model_grid.best_params_\n",
        "    model = clone(SVC(kernel = best['kernel'], C = best['C'], gamma=best['gamma'] ))\n",
        "    model.fit(X_t, y_t)\n",
        "    end = time.perf_counter()\n",
        "    tiempo = end - start\n",
        "    \n",
        "    return model, best, tiempo"
      ],
      "metadata": {
        "id": "1EgBTSlUsAf0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO_yIepczvIx"
      },
      "source": [
        "### Definir la representación y el clasificador\n",
        "\n",
        "Para esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando así nuestra programación.\n",
        "\n",
        "El pipeline más básico que podemos hacer es transformar el dataset a Bag of Words y después usar clasificar el BoW usando NaiveBayes:\n",
        "\n",
        "```python\n",
        "    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n",
        "```\n",
        "\n",
        "Ahora, si queremos usar nuestra transformación para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenará los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n",
        "\n",
        "```python\n",
        "    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n",
        "                                        ('chars_count',CharsCountTransformer())])),\n",
        "              ('clf', MultinomialNB())])\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDbHjXv-zvIx"
      },
      "source": [
        "Recuerden que cada pipeline representa un sistema de clasificación distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podrían solapar resultados. Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.155528Z",
          "start_time": "2020-04-07T15:44:21.149545Z"
        },
        "id": "z_R6tyMCzvIy"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "\n",
        "def get_experiment_0_pipeline():\n",
        "\n",
        "    return Pipeline(\n",
        "        [\n",
        "            (\n",
        "                \"features\",\n",
        "                FeatureUnion(\n",
        "                    [\n",
        "                        (\"bow\", CountVectorizer()),\n",
        "                        (\"chars_count\", CharsCountTransformer()),\n",
        "                    ]\n",
        "                ),\n",
        "            ),\n",
        "            (\"clf\", MultinomialNB()),\n",
        "        ]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_experiment_1_pipeline(C, gamma):\n",
        "\n",
        "    return Pipeline(\n",
        "        [\n",
        "            (\n",
        "                \"features\",\n",
        "                FeatureUnion(\n",
        "                    [\n",
        "                        (\"bow\", CountVectorizer()),\n",
        "                        (\"chars_count\", CharsCountTransformer()),\n",
        "                    ]\n",
        "                ),\n",
        "            ),\n",
        "            (\"svc_clf\", SVC(kernel='rbf', C=C, gamma=gamma, probability=True)),\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "mx5L3pogvEO4"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmMdm98vzvIy"
      },
      "source": [
        "### Ejecutar el pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.167498Z",
          "start_time": "2020-04-07T15:44:21.157540Z"
        },
        "id": "_eX0cEu-zvIz",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def run(dataset, pipeline):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset.\n",
        "    Retorna el modelo ya entrenado más sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test, aún no se transforma de Strings a valores numéricos.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataset.texto,\n",
        "        dataset.clase,\n",
        "        shuffle=True,\n",
        "        test_size=0.33,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    print(f\"# Datos de entrenamiento en dataset: {len(X_train)}\")\n",
        "    print(f\"# Datos de testing en dataset: {len(X_test)}\")\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline).\n",
        "    # En este caso el Bag of Words es el encargado de transformar de Strings a vectores numéricos.\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "\n",
        "    # Evaluamos:\n",
        "    scores = evaluate(predicted_probabilities, y_test, learned_labels)\n",
        "    return pipeline, learned_labels, scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z96C1ZOfzvIz"
      },
      "source": [
        "### Ejecutar el sistema creado por cada train set\n",
        "\n",
        "Este código crea y entrena los 4 sistemas de clasificación y luego los evalua. Para los experimentos, pueden copiar este código variando el pipeline cuantas veces estimen conveniente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.384119Z",
          "start_time": "2020-04-07T15:44:21.170488Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXAxZBdVzvI0",
        "outputId": "64c4d7cf-0cf8-4f82-9833-acf596bedebc",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset: 8183\n",
            "# Datos de testing en dataset: 4031\n",
            "Matriz de confusión\n",
            "[[ 949   84  424]\n",
            " [ 148  429  260]\n",
            " [ 150   43 1544]]\n",
            "\n",
            "Reporte de clasificación:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.76      0.65      0.70      1457\n",
            "        odio       0.77      0.51      0.62       837\n",
            " incivilidad       0.69      0.89      0.78      1737\n",
            "\n",
            "    accuracy                           0.72      4031\n",
            "   macro avg       0.74      0.68      0.70      4031\n",
            "weighted avg       0.73      0.72      0.72      4031\n",
            "\n",
            "Métricas:\n",
            "\n",
            "AUC:  0.876\tKappa: 0.557\tAccuracy: 0.725\n",
            "------------------------------------------------------\n",
            "\n",
            "AUC: 0.876\n",
            "Kappa: 0.557\n",
            "Accuracy: 0.725\n"
          ]
        }
      ],
      "source": [
        "# Creamos el pipeline\n",
        "pipeline = get_experiment_0_pipeline()\n",
        "\n",
        "# Ejecutamos el pipeline sobre el dataset y guardamos el clasificador, las labels aprendidas y los scores obtenidos\n",
        "classifier, learned_labels, scores = run(train, pipeline)\n",
        "\n",
        "# Imprimimos las métricas en el conjunto de testing\n",
        "print(f\"AUC: {scores[0]}\")\n",
        "print(f\"Kappa: {scores[1]}\")\n",
        "print(f\"Accuracy: {scores[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el pipeline\n",
        "pipeline = get_experiment_1_pipeline(C=1, gamma=0.1)\n",
        "\n",
        "# Ejecutamos el pipeline sobre el dataset y guardamos el clasificador, las labels aprendidas y los scores obtenidos\n",
        "classifier, learned_labels, scores = run(train, pipeline)\n",
        "\n",
        "# Imprimimos las métricas en el conjunto de testing\n",
        "print(f\"AUC: {scores[0]}\")\n",
        "print(f\"Kappa: {scores[1]}\")\n",
        "print(f\"Accuracy: {scores[2]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L5MK_-Tw44A",
        "outputId": "332f9493-b977-4424-b855-f77ab8238808"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset: 8183\n",
            "# Datos de testing en dataset: 4031\n",
            "Matriz de confusión\n",
            "[[1052  128  277]\n",
            " [ 254  366  217]\n",
            " [ 394  118 1225]]\n",
            "\n",
            "Reporte de clasificación:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.62      0.72      0.67      1457\n",
            "        odio       0.60      0.44      0.51       837\n",
            " incivilidad       0.71      0.71      0.71      1737\n",
            "\n",
            "    accuracy                           0.66      4031\n",
            "   macro avg       0.64      0.62      0.63      4031\n",
            "weighted avg       0.65      0.66      0.65      4031\n",
            "\n",
            "Métricas:\n",
            "\n",
            "AUC:  0.815\tKappa: 0.455\tAccuracy: 0.656\n",
            "------------------------------------------------------\n",
            "\n",
            "AUC: 0.815\n",
            "Kappa: 0.455\n",
            "Accuracy: 0.656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T19:37:43.169737Z",
          "start_time": "2019-08-21T19:37:43.166744Z"
        },
        "id": "IUKwcde_zvI0"
      },
      "source": [
        "### Predecir los target set y crear la submission\n",
        "\n",
        "Aquí predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.392097Z",
          "start_time": "2020-04-07T15:44:21.386114Z"
        },
        "id": "mWDUoSmbzvI1"
      },
      "outputs": [],
      "source": [
        "def predict_target(dataset, classifier, labels):\n",
        "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
        "    predicted = pd.DataFrame(\n",
        "        classifier.predict_proba(dataset.texto), columns=labels\n",
        "    )\n",
        "\n",
        "    # Agregar ids\n",
        "    predicted[\"id\"] = dataset.id.values\n",
        "    # Reordenar las columnas\n",
        "    predicted = predicted[[\"id\", \"normal\", \"odio\", \"incivilidad\"]]\n",
        "    return predicted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.588573Z",
          "start_time": "2020-04-07T15:44:21.394094Z"
        },
        "id": "5CJ4PTwZzvI1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "predicted_target = {}\n",
        "\n",
        "# Crear carpeta ./predictions\n",
        "if not os.path.exists(\"./predictions\"):\n",
        "    os.mkdir(\"./predictions\")\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree(\"./predictions\")\n",
        "    os.mkdir(\"./predictions\")\n",
        "\n",
        "\n",
        "# Predecir\n",
        "predicted_target = predict_target(target, classifier, learned_labels)\n",
        "\n",
        "# Guardar predicciones en archivos separados.\n",
        "predicted_target.to_csv(\n",
        "    \"./predictions/prediction.txt\", sep=\"\\t\", header=False, index=False\n",
        ")\n",
        "\n",
        "# Crear archivo zip\n",
        "a = shutil.make_archive(\"predictions\", \"zip\", \"./predictions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCAyJIj8nTlU"
      },
      "source": [
        "## **7. Resultados**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAYwupS3naYX"
      },
      "source": [
        "Muestre y analice sus resultados aquí\n",
        "\n",
        "A continuación una tabla de ejemplo para mostrar los resultados. En este caso sólo está el experimento del baseline más otro clasificador, pero ustedes debiesen generar una mayor cantidad de experimentos.\n",
        "\n",
        "| No. | Approach        |               | AUC   | Kappa | Accuracy |\n",
        "| --- | --------------- | ------------- | ----- | ----- | -------- |\n",
        "|     | Features        | Clasifier     |       |       |          |\n",
        "| 0   | bow+chars_count | MultinomialNB | 0.875 | 0.559 | 0.727    |\n",
        "| 1   | tus features    | tu clasifier  |       |       |          |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqlew0iizvI1"
      },
      "source": [
        "## **8. Conclusiones**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFTikGbszZuc"
      },
      "source": [
        "    Escriba aquí sus conclusiones\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}